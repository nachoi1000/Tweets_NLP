{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\usuario\\anaconda3\\lib\\site-packages (0.6.0.20230303)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from snscrape) (4.9.1)\n",
      "Requirement already satisfied: pytz; python_version < \"3.9.0\" in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from snscrape) (2020.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from snscrape) (3.0.12)\n",
      "Requirement already satisfied: lxml in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from snscrape) (4.5.2)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from snscrape) (2.24.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.0.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Requirement already satisfied: googletrans==3.1.0a0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (3.1.0a0)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.1.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.6.20)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "#Install Packages\n",
    "!pip install snscrape\n",
    "!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Translator\n",
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Me quedo con los tweets que no son en respuesta a otro usuario ni que carga un archivo en el tweet.\n",
    "prueba = sntwitter.TwitterSearchScraper('from:CarlosMaslaton since:2022-01-01 until:2022-12-31')\n",
    "\n",
    "tweets = []\n",
    "\n",
    "for tweet in prueba.get_items():\n",
    "    if (tweet.inReplyToUser is None) and (tweet.media is None):\n",
    "        data = [\n",
    "            tweet.date,\n",
    "            tweet.id,\n",
    "            tweet.rawContent,\n",
    "            tweet.user.username,\n",
    "            tweet.likeCount,\n",
    "            tweet.retweetCount,\n",
    "            tweet.inReplyToUser,\n",
    "            tweet.media\n",
    "        ]\n",
    "        tweets.append(data)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets, columns=['date','id','rawContent','username','likeCount','retweetCount','inReplyToUser','media'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: inReplyToUser, dtype: int64)\n",
      "\n",
      "\n",
      "Series([], Name: inReplyToUser, dtype: int64)\n",
      "\n",
      "\n",
      "(2413, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df['inReplyToUser'].value_counts())\n",
    "print('\\n')\n",
    "print(df['inReplyToUser'].value_counts())\n",
    "print('\\n')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TWEETS TRANSLATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trad_tweets(df):\n",
    "    result = {}\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for t in range(10):\n",
    "\n",
    "        for i in tqdm(df.index):\n",
    "\n",
    "            try:\n",
    "                if i in result.keys():\n",
    "                    continue\n",
    "                result[i] = translator.translate(df.loc[i]['rawContent'], src=\"auto\", dest=\"en\").text\n",
    "                if i in errors:\n",
    "                    errors.remove(i)\n",
    "            except:\n",
    "                if i not in errors:\n",
    "                    errors.append(i)\n",
    "\n",
    "        if len(errors) < 1:\n",
    "            break\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2413/2413 [12:47<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "df['rawContent_eng'] = trad_tweets(df).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "      <th>rawContent_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>Señores, el gobierno argentino se va a quedar ...</td>\n",
       "      <td>Gentlemen, the Argentine government is going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409</th>\n",
       "      <td>Una gran ventaja de la República de Haití es q...</td>\n",
       "      <td>A great advantage of the Republic of Haiti is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>@Afederico87 Justicia no.</td>\n",
       "      <td>@Afederico87 Justice no.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>@AlbertoABarcelo Es un cargo que puedo desempe...</td>\n",
       "      <td>@AlbertoABarcelo It is a position that I can c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>El colchón que me ha tocado aquí en Haití es t...</td>\n",
       "      <td>The mattress that has touched me here in Haiti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             rawContent  \\\n",
       "2408  Señores, el gobierno argentino se va a quedar ...   \n",
       "2409  Una gran ventaja de la República de Haití es q...   \n",
       "2410                          @Afederico87 Justicia no.   \n",
       "2411  @AlbertoABarcelo Es un cargo que puedo desempe...   \n",
       "2412  El colchón que me ha tocado aquí en Haití es t...   \n",
       "\n",
       "                                         rawContent_eng  \n",
       "2408  Gentlemen, the Argentine government is going t...  \n",
       "2409  A great advantage of the Republic of Haiti is ...  \n",
       "2410                           @Afederico87 Justice no.  \n",
       "2411  @AlbertoABarcelo It is a position that I can c...  \n",
       "2412  The mattress that has touched me here in Haiti...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['rawContent','rawContent_eng']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataset\n",
    "df.to_csv('masla_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>rawContent</th>\n",
       "      <th>username</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>inReplyToUser</th>\n",
       "      <th>media</th>\n",
       "      <th>rawContent_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30 17:45:11+00:00</td>\n",
       "      <td>1608881901749809153</td>\n",
       "      <td>En una hora, a las 1540, última sesión del año...</td>\n",
       "      <td>CarlosMaslaton</td>\n",
       "      <td>1200</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In an hour, at 1540, last session of the year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-30 10:11:31+00:00</td>\n",
       "      <td>1608767734086762496</td>\n",
       "      <td>Ante consultas de numerosos foristas, quiero r...</td>\n",
       "      <td>CarlosMaslaton</td>\n",
       "      <td>478</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In response to inquiries from numerous forum m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30 10:01:52+00:00</td>\n",
       "      <td>1608765302631976963</td>\n",
       "      <td>El triunfo electoral del PRO-Juntos por el Cam...</td>\n",
       "      <td>CarlosMaslaton</td>\n",
       "      <td>2846</td>\n",
       "      <td>434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The electoral triumph of the PRO-Together for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-30 09:30:02+00:00</td>\n",
       "      <td>1608757293038055424</td>\n",
       "      <td>No se rompan la cabeza los \"investigadores\", a...</td>\n",
       "      <td>CarlosMaslaton</td>\n",
       "      <td>446</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The \"investigators\" should not puzzle over D'A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-30 00:08:48+00:00</td>\n",
       "      <td>1608616052598124544</td>\n",
       "      <td>La potencia de este fin de año en Argentina, q...</td>\n",
       "      <td>CarlosMaslaton</td>\n",
       "      <td>403</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The power of this end of the year in Argentina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date                   id  \\\n",
       "0  2022-12-30 17:45:11+00:00  1608881901749809153   \n",
       "1  2022-12-30 10:11:31+00:00  1608767734086762496   \n",
       "2  2022-12-30 10:01:52+00:00  1608765302631976963   \n",
       "3  2022-12-30 09:30:02+00:00  1608757293038055424   \n",
       "4  2022-12-30 00:08:48+00:00  1608616052598124544   \n",
       "\n",
       "                                          rawContent        username  \\\n",
       "0  En una hora, a las 1540, última sesión del año...  CarlosMaslaton   \n",
       "1  Ante consultas de numerosos foristas, quiero r...  CarlosMaslaton   \n",
       "2  El triunfo electoral del PRO-Juntos por el Cam...  CarlosMaslaton   \n",
       "3  No se rompan la cabeza los \"investigadores\", a...  CarlosMaslaton   \n",
       "4  La potencia de este fin de año en Argentina, q...  CarlosMaslaton   \n",
       "\n",
       "   likeCount  retweetCount  inReplyToUser  media  \\\n",
       "0       1200            20            NaN    NaN   \n",
       "1        478            12            NaN    NaN   \n",
       "2       2846           434            NaN    NaN   \n",
       "3        446            58            NaN    NaN   \n",
       "4        403            26            NaN    NaN   \n",
       "\n",
       "                                      rawContent_eng  \n",
       "0  In an hour, at 1540, last session of the year ...  \n",
       "1  In response to inquiries from numerous forum m...  \n",
       "2  The electoral triumph of the PRO-Together for ...  \n",
       "3  The \"investigators\" should not puzzle over D'A...  \n",
       "4  The power of this end of the year in Argentina...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('masla_tweets.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING - LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THANKS CHATGPT!!!\n",
    "#doc_series = df['rawContent_eng'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "\n",
    "#lemmatized_series = df['rawContent_eng'].apply(lemmatize_text)\n",
    "\n",
    "#How can I use TQDM in this type of apply functions?\n",
    "df['rawContent_eng_lemma'] = df['rawContent_eng'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to generate 2 Vectorizers, one for bi and tri-grams and other just for singular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    max_df=0.8, \n",
    "    min_df=5, \n",
    "    stop_words=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "vectors = vectorizer.fit_transform(df['rawContent_eng_lemma'].str.lower().replace([',','crazy',pattern],\"\", regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "denselist = dense.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_ngram = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    max_df=0.8, \n",
    "    min_df=5,\n",
    "    ngram_range=(1,3),\n",
    "    stop_words=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "vectors_ngram = vectorizer_ngram.fit_transform(df['rawContent_eng_lemma'].str.lower().replace([',','crazy',pattern],\"\", regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_ngram = vectorizer_ngram.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_ngram = vectors_ngram.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "denselist_ngram = dense_ngram.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_list(dense_list, features_names):\n",
    "    all_keywords = []\n",
    "    for description in dense_list:\n",
    "        x = 0\n",
    "        keywords = []\n",
    "        for word in description:\n",
    "            if word > 0:\n",
    "                keywords.append(features_names[x])\n",
    "            x += 1\n",
    "        all_keywords.append(keywords)\n",
    "    return all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_singular = keyword_list(denselist, feature_names)\n",
    "keywords_ngram = keyword_list(denselist_ngram, feature_names_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    In an hour, at 1540, last session of the year ...\n",
      "1    In response to inquiries from numerous forum m...\n",
      "2    The electoral triumph of the PRO-Together for ...\n",
      "Name: rawContent_eng, dtype: object\n",
      "\n",
      "\n",
      "[['god', 'hour', 'let', 'psychologist', 'session', 'want', 'year'], ['12', 'celebrate', 'default', 'forum', 'friend', 'greet', 'inquiry', 'international', 'judaism', 'member', 'new', 'numerous', 'remind', 'response', 'support', 'want', 'year'], ['2015', 'argument', 'base', 'change', 'come', 'country', 'economic', 'electoral', 'fraud', 'good', 'jet', 'like', 'make', 'moral', 'nature', 'pay', 'peronist', 'policy', 'pro', 'propaganda', 'steal', 'time', 'triumph']]\n",
      "\n",
      "\n",
      "[['god', 'god want', 'hour', 'let', 'psychologist', 'session', 'want', 'year'], ['12', 'celebrate', 'default', 'forum', 'forum member', 'friend', 'greet', 'inquiry', 'inquiry numerous', 'inquiry numerous forum', 'international', 'judaism', 'member', 'new', 'numerous', 'numerous forum', 'numerous forum member', 'remind', 'response', 'response inquiry', 'response inquiry numerous', 'support', 'want', 'year'], ['2015', 'argument', 'base', 'change', 'come', 'country', 'economic', 'economic policy', 'electoral', 'fraud', 'good', 'jet', 'like', 'make', 'moral', 'nature', 'pay', 'peronist', 'policy', 'pro', 'propaganda', 'steal', 'time', 'time come', 'triumph']]\n"
     ]
    }
   ],
   "source": [
    "print(df['rawContent_eng'][0:3])\n",
    "print('\\n')\n",
    "print(keywords_singular[0:3])\n",
    "print('\\n')\n",
    "print(keywords_ngram[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING - KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_singular = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=10, n_init=1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_singular.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids_singular = model_singular.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_singular = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shotout: https://www.youtube.com/watch?v=i74DVqMsRWY&list=PL2VXyKi-KpYttggRATQVmgFcQst3z6OlX&index=6\n",
    "with open (\"../topics_kmeans_singular.text\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids_singular[i, :10]:\n",
    "            f.write(' %s' % terms_singular[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ngram = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=10, n_init=1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ngram.fit(vectors_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids_ngram = model_ngram.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_ngram = vectorizer_ngram.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shotout: https://www.youtube.com/watch?v=i74DVqMsRWY&list=PL2VXyKi-KpYttggRATQVmgFcQst3z6OlX&index=6\n",
    "with open (\"../topics_kmeans_ngram.text\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids_ngram[i, :10]:\n",
    "            f.write(' %s' % terms_ngram[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMENSIONALITY REDUCTION - PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features we have working with tf-idf without biagrams are: (2413, 1561)\n",
      "\n",
      "\n",
      "The number of features we have working with tf-idf with biagrams and triagrams are: (2413, 1889)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of features we have working with tf-idf without biagrams are: {vectors.shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"The number of features we have working with tf-idf with biagrams and triagrams are: {vectors_ngram.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't have to scale data cause tfidf values are from 0 to 1.\n",
    "#n_components = 95 is to use the number of features that grab the 95% of the variation of the data.\n",
    "pca_singular = PCA(n_components = 0.95)\n",
    "pca_ngram = PCA(n_components = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_singular = pca_singular.fit_transform(vectors.toarray())\n",
    "X_pca_ngram = pca_ngram.fit_transform(vectors_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS with PCA (I have to use pipeline to do the KMEANS clusters making clusters with PCAdata and data without PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=10, n_init=1)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_singular.fit(X_pca_singular)\n",
    "model_ngram.fit(X_pca_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids_singular = model_singular.cluster_centers_.argsort()[:, ::-1]\n",
    "order_centroids_ngram = model_ngram.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_singular = vectorizer.get_feature_names()\n",
    "terms_ngram = vectorizer_ngram.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../topics_kmeans_singular_PCA.text\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids_singular[i, :10]:\n",
    "            f.write(' %s' % terms_singular[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../topics_kmeans_ngram_PCA.text\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"Cluster {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids_ngram[i, :10]:\n",
    "            f.write(' %s' % terms_ngram[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check cause the top words in boths clusters are numbers working with PCAdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING - NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_singular = NMF(n_components=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_singular.fit(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOPICS SINGULAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC # 0\n",
      "['carlos', 'report', 'yesterday', 'interested', 'say', 'article', 'maslatón', 'note', 'mind', 'https']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC # 1\n",
      "['year', 'good', 'dollar', 'price', 'country', 'inflation', 'world', 'argentine', 'argentina', 'market']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC # 2\n",
      "['kikuchi', 'political', 'larreta', 'sombrilla', '2023', 'avanza', 'libertad', 'la', 'javier', 'milei']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC # 3\n",
      "['care', 'pro', 'express', 'die', 'despite', 'day', 'really', 'want', 'friday', 'love']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC # 4\n",
      "['numerous', 'economic', 'program', 'issue', 'interested', 'today', 'link', 'https', 'radio', 'statement']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in enumerate(nmf_singular.components_):\n",
    "    print(f\"THE TOP 10 WORDS FOR TOPIC # {index}\")\n",
    "    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_ngram = NMF(n_components=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_ngram.fit(vectors_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOPICS NGRAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC # 0\n",
      "['roja', 'calaca roja', '2023', 'sombrilla larreta', 'calaca', 'larreta', 'sombrilla', 'javier milei', 'javier', 'milei']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC # 1\n",
      "['statement today', 'today', 'interested', 'mind', 'link https', 'mind https', 'radio', 'link', 'statement', 'https']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC # 2\n",
      "['good', 'bull', 'dollar', 'price', 'country', 'inflation', 'world', 'argentine', 'argentina', 'market']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC # 3\n",
      "['javier', 'karina', 'kikuchi', 'milei', 'libertad', 'la libertad avanza', 'libertad avanza', 'avanza', 'la libertad', 'la']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC # 4\n",
      "['really', 'wednesday', 'care', 'express', 'despite', 'day', 'want', 'friday', 'friday love', 'love']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in enumerate(nmf_ngram.components_):\n",
    "    print(f\"THE TOP 10 WORDS FOR TOPIC # {index}\")\n",
    "    print([vectorizer_ngram.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.1.1\n",
      "    Uninstalling pip-20.1.1:\n",
      "      Successfully uninstalled pip-20.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Acceso denegado: 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-tvuezj7v\\\\pip.exe'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdbscan\n",
      "  Using cached hdbscan-0.8.29.tar.gz (5.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from hdbscan) (0.23.1)\n",
      "Collecting joblib>=1.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from hdbscan) (0.29.32)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from hdbscan) (1.23.1)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from hdbscan) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->hdbscan) (2.1.0)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (pyproject.toml): started\n",
      "  Building wheel for hdbscan (pyproject.toml): finished with status 'error'\n",
      "Failed to build hdbscan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for hdbscan (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [40 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  creating build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\n",
      "  cythoning hdbscan/_hdbscan_linkage.pyx to hdbscan\\_hdbscan_linkage.c\n",
      "  cythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan\\_hdbscan_boruvka.c\n",
      "  cythoning hdbscan/_hdbscan_reachability.pyx to hdbscan\\_hdbscan_reachability.c\n",
      "  cythoning hdbscan/_prediction_utils.pyx to hdbscan\\_prediction_utils.c\n",
      "  cythoning hdbscan/dist_metrics.pyx to hdbscan\\dist_metrics.c\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-build-env-2gbibq9q\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-install-rfawpmvf\\hdbscan_303b4abf78d84c89928fa1f49d96e7b9\\hdbscan\\_hdbscan_tree.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-build-env-2gbibq9q\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-install-rfawpmvf\\hdbscan_303b4abf78d84c89928fa1f49d96e7b9\\hdbscan\\_hdbscan_linkage.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-build-env-2gbibq9q\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-install-rfawpmvf\\hdbscan_303b4abf78d84c89928fa1f49d96e7b9\\hdbscan\\_hdbscan_boruvka.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-build-env-2gbibq9q\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-install-rfawpmvf\\hdbscan_303b4abf78d84c89928fa1f49d96e7b9\\hdbscan\\_hdbscan_reachability.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-build-env-2gbibq9q\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-install-rfawpmvf\\hdbscan_303b4abf78d84c89928fa1f49d96e7b9\\hdbscan\\_prediction_utils.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-build-env-2gbibq9q\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Usuario\\AppData\\Local\\Temp\\pip-install-rfawpmvf\\hdbscan_303b4abf78d84c89928fa1f49d96e7b9\\hdbscan\\dist_metrics.pxd\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Could not build wheels for hdbscan, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "!pip install hdbscan\n",
    "#!pip install --upgrade git+https://github.com/scikit-learn-contrib/hdbscan.git#egg=hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
